{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOmZwsI1kTrd"
   },
   "source": [
    "# Groq API 써보기\n",
    "\n",
    "Groq의 빠른 추론 엔진을 LangChain에서 체험하는 코드를 소개합니다.    \n",
    "langchain_groq를 통해 쉽게 연결할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "daSSN5ZSetZl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in /config/.local/lib/python3.10/site-packages (0.1.13)\n",
      "Requirement already satisfied: langchain_groq in /config/.local/lib/python3.10/site-packages (0.2.4)\n",
      "Requirement already satisfied: langchain_community in /config/.local/lib/python3.10/site-packages (0.0.29)\n",
      "Requirement already satisfied: pymupdf in /config/.local/lib/python3.10/site-packages (1.25.2)\n",
      "Requirement already satisfied: pypdf in /config/.local/lib/python3.10/site-packages (4.3.1)\n",
      "Requirement already satisfied: bs4 in /config/.local/lib/python3.10/site-packages (0.0.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: numpy<2,>=1 in /config/.local/lib/python3.10/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.33 in /config/.local/lib/python3.10/site-packages (from langchain) (0.1.53)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /config/.local/lib/python3.10/site-packages (from langchain) (0.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /config/.local/lib/python3.10/site-packages (from langchain) (8.5.0)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Downloading aiohttp-3.11.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: langsmith<0.2.0,>=0.1.17 in /config/.local/lib/python3.10/site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /config/.local/lib/python3.10/site-packages (from langchain) (0.6.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
      "Collecting async-timeout<5.0.0,>=4.0.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langchain_groq\n",
      "  Downloading langchain_groq-0.2.3-py3-none-any.whl (14 kB)\n",
      "  Downloading langchain_groq-0.2.2-py3-none-any.whl (14 kB)\n",
      "  Downloading langchain_groq-0.2.1-py3-none-any.whl (14 kB)\n",
      "  Downloading langchain_groq-0.2.0-py3-none-any.whl (14 kB)\n",
      "  Downloading langchain_groq-0.1.10-py3-none-any.whl (14 kB)\n",
      "  Downloading langchain_groq-0.1.9-py3-none-any.whl (14 kB)\n",
      "  Downloading langchain_groq-0.1.8-py3-none-any.whl (14 kB)\n",
      "  Downloading langchain_groq-0.1.6-py3-none-any.whl (14 kB)\n",
      "  Downloading langchain_groq-0.1.5-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in /config/.local/lib/python3.10/site-packages (from langchain_groq) (0.17.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /config/.local/lib/python3.10/site-packages (from bs4) (4.13.1)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.9/241.9 KB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.1/205.1 KB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiohappyeyeballs>=2.3.0\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 KB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 KB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-inspect<1,>=0.4.0 in /config/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /config/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.26.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (4.8.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /config/.local/lib/python3.10/site-packages (from groq<1,>=0.4.1->langchain_groq) (0.27.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Collecting jsonpointer>=1.9\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /config/.local/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.33->langchain) (23.2)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Downloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.3/130.3 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 KB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.27.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /config/.local/lib/python3.10/site-packages (from beautifulsoup4->bs4) (2.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain_groq) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain_groq) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /config/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "Installing collected packages: propcache, orjson, multidict, jsonpointer, frozenlist, async-timeout, aiohappyeyeballs, yarl, requests-toolbelt, jsonpatch, aiosignal, aiohttp, langchain_groq\n",
      "  Attempting uninstall: langchain_groq\n",
      "    Found existing installation: langchain-groq 0.2.4\n",
      "    Uninstalling langchain-groq-0.2.4:\n",
      "      Successfully uninstalled langchain-groq-0.2.4\n",
      "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 async-timeout-4.0.3 frozenlist-1.5.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain_groq-0.1.5 multidict-6.1.0 orjson-3.10.15 propcache-0.2.1 requests-toolbelt-1.0.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain langchain_groq langchain_community pymupdf pypdf bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo6qR14UoZEu"
   },
   "source": [
    "현재 사용 가능한 주요 모델과 API 제한은 아래와 같습니다.\n",
    "\n",
    "\n",
    "| ID                                     | Requests per Minute | Requests per Day | Tokens per Minute | Tokens per Day |\n",
    "|----------------------------------------|---------------------|------------------|-------------------|----------------|\n",
    "| gemma-7b-it                            | 30                  | 14,400           | 15,000            | 500,000        |\n",
    "| gemma2-9b-it                           | 30                  | 14,400           | 15,000            | 500,000        |\n",
    "| llama-3.1-70b-versatile                | 30                  | 14,400           | 18,000            | 500,000        |\n",
    "| llama-3.1-8b-instant                   | 30                  | 14,400           | 20,000            | 500,000        |\n",
    "| llama-3.2-11b-text-preview             | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-11b-vision-preview           | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-1b-preview                   | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-3b-preview                   | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-90b-text-preview             | 30                  | 7,000            | 7,000             | 500,000        |\n",
    "| llama-3.2-90b-vision-preview           | 15                  | 3,500            | 7,000             | 250,000        |\n",
    "| llava-v1.5-7b-4096-preview             | 30                  | 14,400           | 30,000            | (No limit)     |\n",
    "| mixtral-8x7b-32768                     | 30                  | 14,400           | 5,000             | 500,000        |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZozwH3G_kPDc"
   },
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq # Groq-LangChain 연결\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcNUf5TCpZTe"
   },
   "source": [
    "간단한 체인을 만들고 실행합니다.\n",
    "\n",
    "https://console.groq.com/keys 에서 키를 생성해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "# 환경 변수 가져오기\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NVj9S2kZeThO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groq 엔진의 LLM(Large Language Model) 추론 속도가 빠른 이유는 Groq Language Processing Unit (LPU) 기술에 있습니다. LPU는 GPU(GPU)와 달리 AI 추론과 언어 처리를 위해 특별히 설계된 기술입니다.\n",
      "\n",
      "LPU는 다음과 같은 특징을 가지고 있습니다.\n",
      "\n",
      "1. **AI 추론을 위한 설계**: LPU는 AI 추론을 위해 특별히 설계된 기술입니다. GPU는 그래픽 처리를 위해 설계된 기술이기 때문에 AI 추론에 최적화된 설계가 아닙니다.\n",
      "2. **빠른 속도**: LPU는 AI 추론을 위한 빠른 속도를 제공합니다. Groq는 LPU를 사용하여 빠른 AI 추론 속도를 달성할 수 있습니다.\n",
      "3. **저能耗**: LPU는 저能耗을 제공합니다. 이는 AI 추론을 위한 저렴한 비용과 환경 친화적인 설계를 가능하게 합니다.\n",
      "4. **스케일러블**: LPU는 스케일러블 설계를 제공합니다. 이는 많은 양의 데이터를 처리할 수 있는 설계를 가능하게 합니다.\n",
      "\n",
      "Groq 엔진의 LLM 추론 속도가 빠른 이유는 이러한 LPU 기술의 특징 때문입니다. Groq 엔진은 LPU를 사용하여 빠른 AI 추론 속도를 달성할 수 있습니다.\n",
      "Elapsed Time: 0.6871941089630127 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['GROQ_API_KEY'] = groq_api_key\n",
    "\n",
    "\n",
    "chat = ChatGroq(\n",
    "    temperature=0.1,\n",
    "    model=\"llama-3.2-11b-vision-preview\",\n",
    ")\n",
    "\n",
    "url = \"https://wow.groq.com/why-groq/\"\n",
    "loader = WebBaseLoader(url)\n",
    "\n",
    "docs = loader.load()\n",
    "question = \"Groq 엔진의 LLM 추론 속도가 빠른 이유는 무엇인가요? 한국어로 답변하세요.\"\n",
    "\n",
    "\n",
    "system = \"Answer the question from given contexts. Answer in Korean.\"\n",
    "human = \"\"\"\n",
    "Context: {context}\n",
    "\n",
    "---\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat | StrOutputParser()\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "result = chain.invoke(\n",
    "          {\"context\":docs[0].page_content,\n",
    "            'question':question})\n",
    "print(result)\n",
    "\n",
    "end = time.time()\n",
    "elapsed_time = end - start\n",
    "print(f\"Elapsed Time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyM4Yt-fMpoX"
   },
   "source": [
    "## Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A4F1k171MpoY"
   },
   "source": [
    "요약은 LLM의 아주 중요한 기능 중 하나입니다.   \n",
    "일반적으로, LLM의 Abstractive Summarization은 3개의 방법을 사용합니다.\n",
    "\n",
    "- Stuff : 전체 코퍼스를 하나의 프롬프트에 넣고 요약 생성하기\n",
    "- Map-Reduce : 코퍼스를 청크로 분리하고, 각 청크의 요약을 생성한 뒤 합치기\n",
    "- Refine: 코퍼스를 청크로 분리하고, 순차적으로 읽으며 요약 업데이트하기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9vkYtYkMpoa"
   },
   "source": [
    "## Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUemn2D7ikKc"
   },
   "source": [
    "gemma 2 모델을 이용해 요약을 수행해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kMmlgYzWMpoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16722\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Summarize the following paper in Korean.\\nEmphasize the uniqueness and contribution of the paper.\\nAnswer should be in 10 sentences.\\nPlease Answer in Korean.\\n', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='\\n\\nLlama 3.2: Revolutionizing edge AI and vision with open, customizable models\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nOur approachResearchProduct experiencesLlamaBlogTry Meta AIFEATUREDLarge Language Model Llama 3.2: Revolutionizing edge AI and vision with open, customizable modelsSeptember 25, 2024•15 minute readTakeaways:Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B) that fit onto edge and mobile devices, including pre-trained and instruction-tuned versions.The Llama 3.2 1B and 3B models support context length of 128K tokens and are state-of-the-art in their class for on-device use cases like summarization, instruction following, and rewriting tasks running locally at the edge. These models are enabled on day one for Qualcomm and MediaTek hardware and optimized for Arm processors.Supported by a broad ecosystem, the Llama 3.2 11B and 90B vision models are drop-in replacements for their corresponding text model equivalents, while exceeding on image understanding tasks compared to closed models, such as Claude 3 Haiku. Unlike other open multimodal models, both pre-trained and aligned models are available to be fine-tuned for custom applications using torchtune and deployed locally using torchchat. They’re also available to try using our smart assistant, Meta AI.We’re sharing the first official Llama Stack distributions, which will greatly simplify the way developers work with Llama models in different environments, including single-node, on-prem, cloud, and on-device, enabling turnkey deployment of retrieval-augmented generation (RAG) and tooling-enabled applications with integrated safety.We’ve been working closely with partners like AWS, Databricks, Dell Technologies, Fireworks, Infosys, and Together AI to build Llama Stack distributions for their downstream enterprise clients. On-device distribution is via PyTorch ExecuTorch, and single-node distribution is via Ollama.We continue to share our work because we believe openness drives innovation and is good for developers, Meta, and the world. Llama is already leading the way on openness, modifiability, and cost efficiency—enabling more people to have creative, useful, and life-changing breakthroughs using generative AI.We’re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, Snowflake, and more.We’ve been excited by the impact the Llama 3.1 herd of models have made in the two months since we announced them, including the 405B—the first open frontier-level AI model. While these models are incredibly powerful, we recognize that building with them requires significant compute resources and expertise. We’ve also heard from developers who don’t have access to these resources and still want the opportunity to build with Llama. As Meta Founder and CEO Mark Zuckerberg shared today at Connect, they won’t have to wait any longer. Today, we’re releasing Llama 3.2, which includes small and medium-sized vision LLMs (11B and 90B) and lightweight, text-only models (1B and 3B) that fit onto select edge and mobile devices.It’s only been a year and a half since we first announced Llama, and we’ve made incredible progress in such a short amount of time. This year, Llama has achieved 10x growth and become the standard for responsible innovation. Llama also continues to lead on openness, modifiability, and cost efficiency, and it’s competitive with closed models—even leading in some areas. We believe that openness drives innovation and is the right path forward, which is why we continue to share our research and collaborate with our partners and the developer community.We’re making Llama 3.2 models available for download on llama.com and Hugging Face, as well as available for immediate development on our broad ecosystem of partner platforms. Partners are an important part of this work, and we’ve worked with over 25 companies, including AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud, and Snowflake, to enable services on day one. For the Llama 3.2 release, we’re also working with on-device partners Arm, MediaTek, and Qualcomm to offer a broad range of services at launch. Starting today, we’re also making Llama Stack available to the community. More details on the latest release, including information on the multimodal availability in Europe, can be found in our acceptable use policy.Meet Llama 3.2The two largest models of the Llama 3.2 collection, 11B and 90B, support image reasoning use cases, such as document-level understanding including charts and graphs, captioning of images, and visual grounding tasks such as directionally pinpointing objects in images based on natural language descriptions. For example, a person could ask a question about which month in the previous year their small business had the best sales, and Llama 3.2 can then reason based on an available graph and quickly provide the answer. In another example, the model could reason with a map and help answer questions such as when a hike might become steeper or the distance of a particular trail marked on the map. The 11B and 90B models can also bridge the gap between vision and language by extracting details from an image, understanding the scene, and then crafting a sentence or two that could be used as an image caption to help tell the story.The lightweight 1B and 3B models are highly capable with multilingual text generation and tool calling abilities. These models empower developers to build personalized, on-device agentic applications with strong privacy where data never leaves the device. For example, such an application could help summarize the last 10 messages received, extract action items, and leverage tool calling to directly send calendar invites for follow-up meetings.Running these models locally comes with two major advantages. First, prompts and responses can feel instantaneous, since processing is done locally. Second, running models locally maintains privacy by not sending data such as messages and calendar information to the cloud, making the overall application more private. Since processing is handled locally, the application can clearly control which queries stay on the device and which may need to be processed by a larger model in the cloud.Model evaluationsOur evaluation suggests that the Llama 3.2 vision models are competitive with leading foundation models, Claude 3 Haiku and GPT4o-mini on image recognition and a range of visual understanding tasks. The 3B model outperforms the Gemma 2 2.6B and Phi 3.5-mini models on tasks such as following instructions, summarization, prompt rewriting, and tool-use, while the 1B is competitive with Gemma.We evaluated performance on over 150 benchmark datasets that span a wide range of languages. For the vision LLMs, we evaluated performance on benchmarks for image understanding and visual reasoning.Vision modelsAs the first Llama models to support vision tasks, the 11B and 90B models required an entirely new model architecture that supports image reasoning.To add image input support, we trained a set of adapter weights that integrate the pre-trained image encoder into the pre-trained language model. The adapter consists of a series of cross-attention layers that feed image encoder representations into the language model. We trained the adapter on text-image pairs to align the image representations with the language representations. During adapter training, we also updated the parameters of the image encoder, but intentionally did not update the language-model parameters. By doing that, we keep all the text-only capabilities intact, providing developers a drop-in replacement for Llama 3.1 models.Our training pipeline consists of multiple stages, starting from pretrained Llama 3.1 text models. First, we add image adapters and encoders, then pretrain on large-scale noisy (image, text) pair data. Next, we train on medium-scale high quality in-domain and knowledge-enhanced (image, text) pair data.In post-training, we use a similar recipe as the text models by doing several rounds of alignment on supervised fine-tuning, rejection sampling, and direct preference optimization. We leverage synthetic data generation by using the Llama 3.1 model to filter and augment question and answers on top of in-domain images, and use a reward model to rank all the candidate answers to provide high quality fine-tuning data. We also add safety mitigation data to produce a model with a high level of safety while retaining helpfulness of the modeThe end result is a set of models that can take in both image and text prompts, and deeply understand and reason on the combination. This is another step toward Llama models having even richer agentic capabilities.Lightweight modelsAs we talked about with Llama 3.1, powerful teacher models can be leveraged to create smaller models that have improved performance. We used two methods—pruning and distillation—on the 1B and 3B models, making them the first highly capable lightweight Llama models that can fit on devices efficiently.Pruning enabled us to reduce the size of extant models in the Llama herd while recovering as much knowledge and performance as possible. For the 1B and 3B models, we took the approach of using structured pruning in a single shot manner from the Llama 3.1 8B. This involved systematically removing parts of the network and adjusting the magnitude of the weights and gradients to create a smaller, more efficient model that retains the performance of the original network.Knowledge distillation uses a larger network to impart knowledge on a smaller network, with the idea that a smaller model can achieve better performance using a teacher than it could from scratch. For the 1B and 3B in Llama 3.2, we incorporated logits from the Llama 3.1 8B and 70B models into the pre-training stage of the model development, where outputs (logits) from these larger models were used as token-level targets. Knowledge distillation was used after pruning to recover performance.In post-training, we use a similar recipe as Llama 3.1 and produce final chat models by doing several rounds of alignment on top of the pre-trained model. Each round involves supervised fine-tuning (SFT), rejection sampling (RS), and direct preference optimization (DPO).In post-training, we scale context length support to 128K tokens, while maintaining the same quality as the pre-trained model. We also engage in synthetic data generation that goes through careful data processing and filtering to ensure high quality. We carefully blend the data to optimize for high quality across multiple capabilities like summarization, rewriting, instruction following, language reasoning, and tool use.To enable the community to innovate on these models, we worked closely with Qualcomm and Mediatek, the top two mobile system on a chip (SoC) companies in the world, and Arm, who provides the foundational compute platform for 99% of mobile devices. The weights being released today are based on BFloat16 numerics. Our teams are actively exploring quantized variants that will run even faster, and we hope to share more on that soon.This demo is based on an unreleased quantized model.This demo is based on an unreleased quantized model.Llama Stack distributionsIn July, we released a request for comment on the Llama Stack API, a standardized interface for canonical toolchain components (fine-tuning, synthetic data generation) to customize Llama models and build agentic applications. The engagement has been great.Since then, we have been working hard to make the API real. We built a reference implementation of the APIs for inference, tool use, and RAG. In addition, we have been working with partners to adapt them to become providers for the APIs. Finally, we have introduced Llama Stack Distribution as a way to package multiple API Providers that work well together to provide a single endpoint for developers. We are now sharing with the community a simplified and consistent experience that will enable them to work with Llama models in multiple environments, including on-prem, cloud, single-node, and on-device.The full set of releases includes:Llama CLI (command line interface) to build, configure, and run Llama Stack distributionsClient code in multiple languages, including python, node, kotlin, and swiftDocker containers for Llama Stack Distribution Server and Agents API ProviderMultiple distributionsSingle-node Llama Stack Distribution via Meta internal implementation and OllamaCloud Llama Stack distributions via AWS, Databricks, Fireworks, and TogetherOn-device Llama Stack Distribution on iOS implemented via PyTorch ExecuTorchOn-prem Llama Stack Distribution supported by DellWe look forward to working with developers and partners to simplify all aspects of building with Llama models and welcome feedback.System level safetyTaking an open approach has many benefits. It helps ensure that more people around the world can access the opportunities that AI provides, guards against concentrating power in the hands of a small few, and deploys technology more equitably and safely across society. As we continue to innovate, we also want to make sure we’re empowering developers to build safe and responsible systems.Building on our previous release and continuous effort to support responsible innovation, today we’re adding new updates to our family of safeguards:First, we’re releasing Llama Guard 3 11B Vision, which is designed to support Llama 3.2’s new image understanding capability and filter text+image input prompts or text output responses to these prompts.Second, as we released 1B and 3B Llama models to be used in more constrained environments like on-device, we also optimized Llama Guard to drastically reduce its deployment cost. Llama Guard 3 1B is based on the Llama 3.2 1B model and has been pruned and quantized bringing its size from 2,858 MB down to 438 MB, making it more efficient than ever to deploy.These new solutions are integrated into our reference implementations, demos, and applications and are ready for the open source community to use on day one.Try Llama 3.2 todayLlama 3.2 is poised to reach more people than ever before and enable exciting new use cases. We believe sharing these models with the open source community isn’t enough. We want to make sure developers also have the tools they need to build with Llama responsibly. As part of our continued responsible release efforts, we’re offering developers new tools and resources, and as always, we’ll update best practices in our Responsible Use Guide.We continue to share the latest advancements in the Llama ecosystem because we believe openness drives innovation and is good for developers, Meta, and the world. We’re excited to continue the conversations we’re having with our partners and the open source community, and as always, we can’t wait to see what the community builds using Llama 3.2 and Llama Stack.This work was supported by our partners across the AI community. We’d like to thank and acknowledge (in alphabetical order): Accenture, AMD, Arm, AWS, Cloudflare, Databricks, Dell, Deloitte, Fireworks.ai, Google Cloud, Groq, Hugging Face, IBM watsonx, Infosys, Intel, Kaggle, Lenovo, LMSYS, MediaTek, Microsoft Azure, NVIDIA, OctoAI, Ollama, Oracle Cloud, PwC, Qualcomm, Sarvam AI, Scale AI, Snowflake, Together AI, and UC Berkeley - vLLM Project.Learn more on the Llama websiteVisit Hugging FaceShare:Our latest updates delivered to your inboxSubscribe to our newsletter to keep up with Meta AI news, events, research breakthroughs, and more.Join us in the pursuit of what’s possible with AI.See all open positionsRelated PostsResponsible AIConnect 2024: The responsible approach we’re taking to generative AI September 25, 2024Read postWith 10x growth since 2023, Llama is the leading engine of AI innovationAugust 29, 2024Read postOpen SourceGenerate an entire app from a prompt using Together AI’s LlamaCoderSeptember 18, 2024Read postOur approachAbout AI at MetaResponsibilityPeopleCareersResearchInfrastructureResourcesDemosProduct experiencesMeta AIAI StudioLatest newsBlogNewsletterFoundational modelsLlamaOur approachOur approachAbout AI at MetaResponsibilityPeopleCareersResearchResearchInfrastructureResourcesDemosProduct experiencesMeta AIAI StudioLatest newsLatest newsBlogNewsletterFoundational modelsLlamaPrivacy PolicyTermsCookies Meta © 2025\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stuff\n",
    "example_URL='https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/'\n",
    "\n",
    "loader = WebBaseLoader(example_URL)\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "gemma2 = ChatGroq(\n",
    "    temperature=0,\n",
    "    model=\"gemma2-9b-it\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "summarize_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''Summarize the following paper in Korean.\n",
    "Emphasize the uniqueness and contribution of the paper.\n",
    "Answer should be in 10 sentences.\n",
    "Please Answer in Korean.\n",
    "'''),\n",
    "    ('user', '''{text}''')])\n",
    "\n",
    "print(len(docs[0].page_content))\n",
    "summarize_prompt.format_messages(text=docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5MOx1k71Mpoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Llama 3.2: 핵심 내용 요약 (10문장)\n",
      "\n",
      "Meta는 오픈소스, 맞춤형 모델인 Llama 3.2를 출시하여 에지 AI 및 비전 분야에 혁명을 가져올 예정입니다. Llama 3.2는 11B 및 90B 크기의 비전 LLMs와 1B 및 3B 크기의 가벼운 텍스트만 지원하는 모델을 포함합니다. 이 모델들은 Qualcomm 및 MediaTek 하드웨어와 호환되며 Arm 프로세서를 위한 최적화된 버전을 제공합니다. Llama 3.2는 텍스트 모델과 동일한 크기의 11B 및 90B 비전 모델을 제공하며, Claude 3 Haiku와 같은 폐쇄형 모델보다 이미지 이해 능력이 우수합니다. 또한, Llama Guard 3 11B Vision과 같은 새로운 안전 기능을 통해 책임감 있는 AI 개발을 지원합니다. Llama Stack 분포를 통해 개발자들은 다양한 환경에서 Llama 모델을 사용하기 쉽게 만들었습니다. 이 분포는 싱글 노드, 클라우드, 온-프레미스, 에지 기기 등 다양한 환경에서 사용 가능합니다. Llama 3.2는 Hugging Face 및 llama.com에서 다운로드 가능하며, AMD, AWS, Databricks, Dell, Google Cloud, Groq, IBM, Intel, Microsoft Azure, NVIDIA, Oracle Cloud 등 다양한 파트너 플랫폼에서 즉시 개발할 수 있습니다. Meta는 Llama 3.2를 통해 오픈소스, 수정 가능성, 비용 효율성을 강조하며, 개발자, Meta, 그리고 전 세계에 혁신을 가져올 것이라고 믿습니다.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_chain = summarize_prompt | gemma2 | StrOutputParser()\n",
    "summary = summarize_chain.invoke(docs[0].page_content)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCaANqOKMpoa"
   },
   "source": [
    "해당 방법은 매우 간단하지만, Context 길이를 넘어서는 경우 에러가 발생합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLO3VmBdikKd"
   },
   "source": [
    "## Map-Reduce\n",
    "LangChain의 PyMuPdfLoader를 이용하여 임의의 PDF를 요약해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "14r339tyikKd"
   },
   "outputs": [],
   "source": [
    "# # Password 있는 PDF 열기\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "path_material = './교재.pdf'\n",
    "\n",
    "# pypdf_loader = PyPDFLoader(path_material, password='비밀번호')\n",
    "# material_pages = pypdf_loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "BqWN9mrkikKd"
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "import urllib.request\n",
    "\n",
    "paper_URL = \"https://arxiv.org/pdf/2410.05983\"\n",
    "\n",
    "\n",
    "# 외부 링크에서 PDF 파일을 다운로드하는 코드\n",
    "urllib.request.urlretrieve(\n",
    "    paper_URL,\n",
    "    filename=\"paper.pdf\"\n",
    ")\n",
    "path = './paper.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "ImhjYgkqikKd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97331"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "loader = PyMuPDFLoader(path)\n",
    "# 페이지별로 저장\n",
    "pages = loader.load()\n",
    "\n",
    "# 코퍼스에 모두 결합\n",
    "corpus = Document(page_content='')\n",
    "for page in pages:\n",
    "    corpus.page_content += page.page_content\n",
    "len(corpus.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvhDiGhdMpoa"
   },
   "source": [
    "긴 Context를 처리하기 위해, 전체 코퍼스를 작은 단위로 쪼개 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "c5pXmAy-Mpoa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
    "document_list = text_splitter.split_documents([corpus])\n",
    "len(document_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UE34hmK8Mpoa"
   },
   "source": [
    "이후 Map-Reduce와 Refine을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1rP6f8OMpoa"
   },
   "source": [
    "## Map-Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "itfSciDVMpoa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1/11 [00:00<00:09,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 0\n",
      "본 논문은 장문맥 LLM(Long-Context LLMs)을 사용한 Retrieval-Augmented Generation(RAG) 시스템의 새로운 과제와 해결책을 제시합니다. 장문맥 LLM은 더 많은 정보를 처리할 수 있지만, 과도한 정보량은 LLM의 생성 성능 저하로 이어질 수 있습니다. 특히, 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM의 생성을 혼란스럽게 만들 수 있습니다. 이 논문에서는 장문맥 LLM을 사용한 RAG 시스템에서 hard negatives의 부정적인 영향을 분석하고, 이를 해결하기 위한 세 가지 새로운 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM이 더욱 관련성 있는 정보에 집중하도록 유도하는 방법입니다. 둘째, hard negatives에 대한 내성을 갖도록 LLM을 훈련하는 방법입니다. 셋째, LLM이 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 훈련하는 방법입니다. 이러한 방법들은 데이터 분포, 사용된 검색 알고리즘, 훈련 문맥 길이 등 다양한 요소를 고려하여 장문맥 LLM 기반 RAG 시스템의 성능을 향상시키는 데 기여합니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 2/11 [00:02<00:09,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 1\n",
      "본 논문은 장문맥 LLM(Long-Context LLMs)을 기반으로 한 RAG(Retrieval-Augmented Generation) 시스템에서 발생하는 과제들을 분석하고 해결 방안을 제시합니다. \n",
      "\n",
      "첫째, 장문맥 LLM이 처리할 수 있는 답변 텍스트의 길이가 증가함에 따라 RAG 성능이 향상되는지, 또는 오히려 감소하는지에 대한 질문을 던지며, 다양한 LLM과 검색기(retriever)를 사용하여 실험을 진행합니다. 실험 결과, 검색기의 성능이 높을수록 장문맥 LLM의 성능은 초기에는 증가하지만, 특정 범위를 넘어서면 감소하는 \"역U자형\" 패턴을 보이는 것으로 나타났습니다. 이는 검색기가 너무 많은 정보를 가져오면 LLM이 오히려 혼란스러워지고 성능이 저하될 수 있음을 시사합니다.\n",
      "\n",
      "둘째, 검색기의 성능과 LLM의 처리 능력 사이의 상호 작용을 분석합니다. 높은 재현율(recall)을 가진 검색기와 낮은 재현율을 가진 검색기 모두 사용하여 실험을 진행하고, LLM의 성능과 검색 결과의 재현율, 정확도(precision) 사이의 관계를 살펴봅니다. 결과적으로, 높은 재현율을 가진 검색기는 더 많은 정보를 가져오지만, 그 정보 중 일부가 오히려 LLM의 성능을 저하시키는 \"hard negatives\"로 작용하는 것으로 나타났습니다. 즉, 단순히 검색 결과의 양이 많을수록 LLM의 성능이 향상되는 것은 아니며, 검색 결과의 질과 LLM의 처리 능력 사이의 균형이 중요합니다.\n",
      "\n",
      "이러한 분석을 통해 논문은 장문맥 LLM을 활용한 RAG 시스템에서 발생하는 핵심 과제들을 명확히 제시하고, 이를 해결하기 위한 방향을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 3/11 [00:03<00:08,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 2\n",
      "본 논문은 긴 텍스트 컨텍스트에서의 RAG(Retrieval Augmented Generation) 시스템에서 긴 텍스트를 처리하는 대규모 언어 모델(LLM)의 성능을 향상시키는 방법을 연구합니다. \n",
      "\n",
      "첫째, 논문은 기존 평가 지표인 정확도가 긴 텍스트 컨텍스트에서 LLM 성능을 완벽하게 반영하지 못한다는 점을 보여줍니다. 특히, \"hard negatives\" (관련성이 낮은 텍스트)가 LLM의 성능에 큰 영향을 미치며, 이를 정확도만으로는 측정하기 어렵다는 것을 강조합니다. \n",
      "\n",
      "둘째, 논문은 \"hard negatives\"가 LLM 성능에 미치는 영향을 규명하기 위해 다양한 retriever(정보 검색 알고리즘)와 LLM 조합을 사용한 실험을 수행합니다. 실험 결과, retriever의 성능이 높을수록 \"hard negatives\"의 영향이 더욱 커지는 것을 확인했습니다. \n",
      "\n",
      "셋째, 논문은 \"hard negatives\"의 문제를 해결하기 위해 \"retrieval reordering\" (정보 검색 결과의 순서 재배치)라는 새로운 방법을 제안합니다. 이 방법은 LLM이 \"lost-in-the-middle\" 현상(중간 정보에 대한 주의력 부족)을 극복하고, 가장 관련성이 높은 정보를 먼저 처리하도록 유도하여 \"hard negatives\"의 영향을 줄입니다. \n",
      "\n",
      "마지막으로, 논문은 \"data-augmented fine-tuning\" (데이터 증강을 통한 fine-tuning)을 통해 LLM의 \"hard negatives\"에 대한 내성을 향상시킬 수 있다는 가능성을 제시합니다. 이 방법은 LLM을 다양한 컨텍스트에서 학습시켜 \"hard negatives\"를 효과적으로 처리하도록 돕습니다. \n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 4/11 [00:04<00:08,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 3\n",
      "본 논문은 Retrieval Augmented Generation(RAG)에 적합한 대규모 언어 모델(LLM)을 위한 데이터 증강 미세 조정 방법을 제시하고 그 효과를 분석합니다. \n",
      "\n",
      "첫째, 논문은 단순히 질문과 답변 쌍으로 미세 조정하는 것보다, 질문, 지시, 검색된 텍스트 패스지, 질문을 입력으로 받아 답변을 생성하는 RAG 특화 미세 조정을 통해 LLM의 견고성을 향상시킬 수 있다고 주장합니다. 이 방법은 다양한 검색된 텍스트를 학습시켜 LLM이 노이즈 속에서도 관련 정보를 효과적으로 식별하고 활용할 수 있도록 돕습니다. 실험 결과, RAG 특화 미세 조정은 검색된 텍스트의 수가 증가함에 따라 성능이 더욱 안정적으로 향상되는 것을 보여주며, 일반적인 질문-답변 미세 조정보다 우수한 성능을 보입니다.\n",
      "\n",
      "둘째, 논문은 LLM이 검색된 텍스트에서 관련 정보를 명확하게 식별하고 활용할 수 있도록, 미세 조정 과정에 중간 단계로 추론 단계를 추가하는 방법을 제안합니다. LLM은 질문과 검색된 텍스트를 입력으로 받아 관련 정보를 명시적으로 식별하는 추론 문단과 답변을 생성합니다. 이 방법은 LLM이 검색된 텍스트를 구조화된 방식으로 처리하고 관련 정보를 더욱 효과적으로 파악할 수 있도록 돕습니다. 실험 결과, 추론 단계를 추가한 미세 조정은 단순히 RAG 특화 미세 조정보다 더욱 향상된 성능을 보여줍니다.\n",
      "\n",
      "마지막으로, 논문은 RAG 특화 미세 조정의 효과를 높이기 위해 다양한 데이터 분포, 검색 알고리즘, 학습 텍스트 길이 등을 조사합니다. 다양한 데이터 분포를 학습시키면 LLM의 일반화 능력이 향상되며, 여러 검색 알고리즘을 사용하여 미세 조정하면 새로운 검색 알고리즘에서도 좋은 성능을 보입니다. 또한, 최대 텍스트 길이를 학습 범위로 설정하면 다양한 검색된 텍스트 수에 대해서도 안정적인 성능을 보입니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 5/11 [00:05<00:07,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 4\n",
      "본 논문은 긴 텍스트 컨텍스트를 가진 대규모 언어 모델(LLM)이 검색 증강 생성(RAG) 시스템에서 장거리 입력에 대한 과제를 해결하는 방법을 연구합니다. 기존 연구와 달리, 본 논문은 많은 검색 결과를 사용하는 것이 성능 향상에 도움이 되지 않고 오히려 성능 저하를 초래할 수 있다는 것을 발견했습니다. 이는 검색된 \"hard negatives\"가 LLM의 성능에 부정적인 영향을 미치기 때문입니다. 이 문제를 해결하기 위해, 본 논문에서는 학습 전 검색 결과 재정렬, RAG 특화 암묵적 LLM 미세 조정, 그리고 중간 추론을 포함한 RAG 지향적 LLM 미세 조정 등 세 가지 해결책을 제안하고 평가합니다. 또한, 데이터 분포, 학습에 사용되는 검색 엔진, 그리고 학습 컨텍스트 길이와 같은 요소들이 RAG 성능에 미치는 영향을 분석합니다. \n",
      "\n",
      "본 논문은 다음과 같은 주요 내용을 다룹니다.\n",
      "\n",
      "* 많은 검색 결과를 사용하는 것이 LLM 성능을 향상시키지 않고 오히려 저하시킬 수 있다는 것을 발견했습니다.\n",
      "* 이러한 성능 저하의 원인은 검색된 \"hard negatives\" 때문임을 밝혔습니다.\n",
      "* 학습 전 검색 결과 재정렬, RAG 특화 암묵적 LLM 미세 조정, 그리고 중간 추론을 포함한 RAG 지향적 LLM 미세 조정 등 세 가지 해결책을 제안했습니다.\n",
      "* 데이터 분포, 학습에 사용되는 검색 엔진, 그리고 학습 컨텍스트 길이와 같은 요소들이 RAG 성능에 미치는 영향을 분석했습니다.\n",
      "* 미래 연구 방향으로는 더욱 고급화된 검색 결과 정렬 방법을 사용한 자동화된 위치 최적화와 다단계 추론 체인을 위한 LLM 미세 조정을 제시했습니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 6/11 [00:19<00:26,  5.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 5\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)이 Retrieval Augmented Generation(RAG)과 같은 기존 기법과 어떻게 상호 작용하는지, 그리고 장문 입력에 대한 RAG의 새로운 가능성을 탐구합니다. \n",
      "\n",
      "첫째, 연구자들은 장문 맥락 LLM이 RAG에 적용될 때 발생하는 새로운 과제들을 분석합니다. 특히, 장문 맥락 LLM은 많은 양의 정보를 처리해야 하기 때문에, 검색된 텍스트의 양과 질이 RAG 성능에 큰 영향을 미칠 수 있습니다. \n",
      "\n",
      "둘째, 연구자들은 다양한 LLM과 검색 알고리즘을 사용하여 NQ와 PopQA와 같은 데이터셋에서 실험을 수행합니다. 실험 결과, 강력한 검색 알고리즘(e5)을 사용할 때 LLM은 초기에는 성능이 향상되지만, 검색된 텍스트의 양이 증가함에 따라 성능이 감소하는 경향을 보입니다. 반면, 약한 검색 알고리즘(BM25)을 사용할 때는 검색된 텍스트의 양이 증가함에 따라 성능이 지속적으로 향상됩니다. \n",
      "\n",
      "셋째, 연구자들은 LLM이 \"hard negative\" (정답이 없는 관련성 있는 텍스트)에 취약하다는 것을 발견합니다. 특히, 강력한 검색 알고리즘으로 검색된 \"hard negative\"는 LLM의 성능에 더 큰 영향을 미칩니다. \n",
      "\n",
      "넷째, 연구자들은 장문 맥락 LLM의 RAG 성능을 향상시키기 위한 몇 가지 방법을 제안합니다. 예를 들어, \"hard negative\"를 제거하거나, 검색된 텍스트의 질을 향상시키는 방법을 고려할 수 있습니다. \n",
      "\n",
      "마지막으로, 연구자들은 장문 맥락 LLM과 RAG의 잠재력을 강조하며, 앞으로 이 분야에서 더 많은 연구가 필요하다고 주장합니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▎   | 7/11 [00:34<00:34,  8.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 6\n",
      "본 논문은 장문 텍스트 입력에 대한 Retrieval Augmented Generation(RAG) 시스템의 성능을 향상시키기 위한 방법을 제시합니다. 특히, 장문 텍스트를 효과적으로 처리하고 이해하기 위해 Long-Context LLMs(대용량 언어 모델)을 RAG에 적용하는 방법에 초점을 맞춥니다. \n",
      "\n",
      "논문은 첫째, 장문 텍스트 입력에 대한 RAG 시스템의 기존 문제점을 분석합니다. 둘째, 이러한 문제점을 해결하기 위해 Long-Context LLMs을 활용한 새로운 RAG 프레임워크를 제안합니다. \n",
      "\n",
      "셋째, 제안된 프레임워크가 다양한 데이터셋에서 기존 RAG 시스템보다 우수한 성능을 보여준다는 것을 실험을 통해 입증합니다. 넷째, Long-Context LLMs의 장점을 활용하여 질문에 대한 답변을 생성하는 과정에서 더욱 정확하고 관련성 있는 결과를 얻을 수 있다는 것을 보여줍니다. 다섯째, 논문은 Hard Negative 문제를 다루며, LLMs가 답변과 관련이 없는 텍스트에 영향을 받지 않도록 하는 방법을 제시합니다. 마지막으로, Long-Context LLMs를 RAG에 적용함으로써 정보 검색 및 질문 답변 시스템의 성능을 향상시킬 수 있다는 결론을 내립니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 8/11 [00:45<00:27,  9.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 7\n",
      "본 논문은 긴 입력에 대한 RAG(Retrieval Augmented Generation)의 새로운 접근 방식을 제시하며, 특히 긴 텍스트 컨텍스트에서의 성능 향상에 초점을 맞춥니다. \n",
      "\n",
      "첫째, 논문은 긴 텍스트 컨텍스트에서의 RAG의 어려움을 분석하고, 이를 극복하기 위한 다양한 방법을 제안합니다. 둘째, 논문은 긴 입력에 대한 RAG 모델의 성능을 향상시키기 위해 새로운 데이터셋과 훈련 방법을 소개합니다. 셋째, 논문은 제안된 방법이 기존 방법에 비해 뛰어난 성능을 보여준다는 것을 실험 결과를 통해 입증합니다. 넷째, 논문은 긴 텍스트 컨텍스트에서의 RAG 모델의 성능 향상을 위한 추가적인 연구 방향을 제시합니다. 다섯째, 논문은 긴 입력에 대한 RAG 모델의 성능 향상을 위한 실용적인 방법을 제공합니다. 마지막으로, 논문은 긴 텍스트 컨텍스트에서의 RAG 모델의 성능 향상에 대한 심층적인 분석을 제공합니다. \n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 9/11 [00:57<00:20, 10.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 8\n",
      "본 논문은 Long-Context LLMs를 활용한 RAG(Retrieval Augmented Generation) 모델의 성능 향상을 다룹니다. 특히, 중간 단계의 추론 능력을 강화하여 질문에 대한 답변을 생성하는 데 초점을 맞춥니다. \n",
      "\n",
      "본 논문은 다양한 데이터셋(NQ, Wizard of Wikipedia, FEVER, MMLU)을 사용하여 RAG 모델을 학습하고 평가합니다. 학습 과정에서, 모델은 질문과 관련된 문서를 분석하고, 답변을 생성하기 위한 논리적 근거를 제시하는 능력을 향상시키도록 지도 학습됩니다. \n",
      "\n",
      "평가 결과, 중간 단계 추론을 활용한 RAG 모델은 기존의 RAG 모델보다 질문에 대한 답변의 정확도와 완전성을 향상시키는 것을 보여줍니다. \n",
      "\n",
      "특히, 긴 텍스트 입력에 대한 처리 능력이 향상되었으며, 복잡한 질문에 대한 답변을 생성하는 데에도 효과적임을 확인했습니다. \n",
      "\n",
      "논문은 또한 데이터 증강 기법을 활용하여 RAG 모델의 성능을 더욱 향상시킬 수 있는 가능성을 제시합니다. \n",
      "\n",
      "결론적으로, 본 논문은 Long-Context LLMs를 활용한 RAG 모델의 성능 향상을 위한 새로운 접근 방식을 제시하며, 긴 텍스트 입력 처리 및 복잡한 질문 답변에 대한 잠재력을 보여줍니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10/11 [01:08<00:10, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 9\n",
      "본 논문은 Gemma-2-9B와 같은 대규모 언어 모델(LLM)을 사용하여 질의응답(RAG) 시스템을 개선하는 방법에 대해 다룹니다. 특히, 데이터 증강된 RAG fine-tuning 기법이 LLM의 성능을 향상시키는 데 효과적임을 보여줍니다. \n",
      "\n",
      "첫째, 논문은 다양한 데이터셋에서 Gemma-2-9B 모델을 RAG fine-tuning하여 그 성능을 평가합니다. RAG fine-tuning은 일반적인 챗 모델보다 질의응답 작업에서 더 높은 정확도를 보입니다. 둘째, 논문은 RAG fine-tuning에 중간 단계의 추론을 추가하여 모델의 성능을 더욱 향상시키는 방법을 제시합니다. 이 방법은 복잡한 질문에 대한 답변을 생성하는 데 유용합니다. 셋째, 논문은 Mistral-Nemo-12B 모델과 같은 다른 LLM에서도 데이터 증강된 RAG fine-tuning 기법이 효과적임을 보여줍니다. 넷째, 논문은 RAG fine-tuning이 LLM의 장점을 활용하여 질의응답 시스템의 성능을 향상시키는 데 유용한 방법임을 강조합니다. 마지막으로, 논문은 앞으로의 연구 방향으로 데이터 증강 기법의 다양한 적용 및 더욱 복잡한 질문에 대한 답변을 생성하는 방법에 대한 연구를 제안합니다.\n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [01:21<00:00,  7.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# 10\n",
      "본 논문은 긴 텍스트 입력에 대한 RAG(Retrieval Augmented Generation)의 효율성을 높이기 위해 Long-Context LLM(Large Language Model)을 사용하는 방법을 연구합니다. \n",
      "\n",
      "첫째, Mistral-Nemo-12B와 Gemini-1.0-Pro 모델을 사용하여 RAG-specific fine-tuning의 효과를 보여줍니다. \n",
      "\n",
      "둘째, RAG-specific fine-tuning 데이터의 크기가 LLM 성능에 미치는 영향을 분석하여 데이터 규모가 증가함에 따라 성능이 향상됨을 확인합니다. \n",
      "\n",
      "셋째, 일반적인 SFT(Supervised Fine-Tuning) 데이터와 RAG-specific 데이터를 결합하여 LLM을 학습시키고, 이 방법이 RAG 성능을 향상시키면서도 일반적인 언어 처리 능력을 유지할 수 있음을 보여줍니다. \n",
      "\n",
      "결론적으로, 본 논문은 긴 텍스트 입력을 처리하는 RAG 시스템에서 Long-Context LLM을 활용하고, RAG-specific fine-tuning과 데이터 규모 조절을 통해 성능을 향상시킬 수 있는 방법을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "===========================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Map 과정 : 각 문서에 대해 요약을 생성합니다. -> Long Context 인 경우, 균일하게 분할하여 요약하고 다시 합치는 방법으로 성능 개선\n",
    "\n",
    "map_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''주어진 논문의 내용을 읽고 한국어로 요약하세요.\n",
    "요약은 1개의 문단과 문단별 6개의 문장으로 작성하세요.\n",
    "답변은 한국어로 작성하세요.\n",
    "같은 내용을 반복하지 마세요.\n",
    "'''),\n",
    "    ('user', '''{text}''')])\n",
    "\n",
    "map_chain  = map_prompt | gemma2 | StrOutputParser()\n",
    "\n",
    "raw_summaries = []\n",
    "for i in tqdm(range(len(document_list))):\n",
    "    response = map_chain.invoke(document_list[i].page_content)\n",
    "    raw_summaries.append(response)\n",
    "    print('')\n",
    "    print('#',i)\n",
    "    print(response)\n",
    "    print('===========================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['본 논문은 장문맥 LLM(Long-Context LLMs)을 사용한 Retrieval-Augmented Generation(RAG) 시스템의 새로운 과제와 해결책을 제시합니다. 장문맥 LLM은 더 많은 정보를 처리할 수 있지만, 과도한 정보량은 LLM의 생성 성능 저하로 이어질 수 있습니다. 특히, 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM의 생성을 혼란스럽게 만들 수 있습니다. 이 논문에서는 장문맥 LLM을 사용한 RAG 시스템에서 hard negatives의 부정적인 영향을 분석하고, 이를 해결하기 위한 세 가지 새로운 방법을 제안합니다. 첫째, 검색된 문서의 순서를 재정렬하여 LLM이 더욱 관련성 있는 정보에 집중하도록 유도하는 방법입니다. 둘째, hard negatives에 대한 내성을 갖도록 LLM을 훈련하는 방법입니다. 셋째, LLM이 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 훈련하는 방법입니다. 이러한 방법들은 데이터 분포, 사용된 검색 알고리즘, 훈련 문맥 길이 등 다양한 요소를 고려하여 장문맥 LLM 기반 RAG 시스템의 성능을 향상시키는 데 기여합니다.\\n\\n\\n',\n",
       " '본 논문은 장문맥 LLM(Long-Context LLMs)을 기반으로 한 RAG(Retrieval-Augmented Generation) 시스템에서 발생하는 과제들을 분석하고 해결 방안을 제시합니다. \\n\\n첫째, 장문맥 LLM이 처리할 수 있는 답변 텍스트의 길이가 증가함에 따라 RAG 성능이 향상되는지, 또는 오히려 감소하는지에 대한 질문을 던지며, 다양한 LLM과 검색기(retriever)를 사용하여 실험을 진행합니다. 실험 결과, 검색기의 성능이 높을수록 장문맥 LLM의 성능은 초기에는 증가하지만, 특정 범위를 넘어서면 감소하는 \"역U자형\" 패턴을 보이는 것으로 나타났습니다. 이는 검색기가 너무 많은 정보를 가져오면 LLM이 오히려 혼란스러워지고 성능이 저하될 수 있음을 시사합니다.\\n\\n둘째, 검색기의 성능과 LLM의 처리 능력 사이의 상호 작용을 분석합니다. 높은 재현율(recall)을 가진 검색기와 낮은 재현율을 가진 검색기 모두 사용하여 실험을 진행하고, LLM의 성능과 검색 결과의 재현율, 정확도(precision) 사이의 관계를 살펴봅니다. 결과적으로, 높은 재현율을 가진 검색기는 더 많은 정보를 가져오지만, 그 정보 중 일부가 오히려 LLM의 성능을 저하시키는 \"hard negatives\"로 작용하는 것으로 나타났습니다. 즉, 단순히 검색 결과의 양이 많을수록 LLM의 성능이 향상되는 것은 아니며, 검색 결과의 질과 LLM의 처리 능력 사이의 균형이 중요합니다.\\n\\n이러한 분석을 통해 논문은 장문맥 LLM을 활용한 RAG 시스템에서 발생하는 핵심 과제들을 명확히 제시하고, 이를 해결하기 위한 방향을 제시합니다. \\n\\n\\n',\n",
       " '본 논문은 긴 텍스트 컨텍스트에서의 RAG(Retrieval Augmented Generation) 시스템에서 긴 텍스트를 처리하는 대규모 언어 모델(LLM)의 성능을 향상시키는 방법을 연구합니다. \\n\\n첫째, 논문은 기존 평가 지표인 정확도가 긴 텍스트 컨텍스트에서 LLM 성능을 완벽하게 반영하지 못한다는 점을 보여줍니다. 특히, \"hard negatives\" (관련성이 낮은 텍스트)가 LLM의 성능에 큰 영향을 미치며, 이를 정확도만으로는 측정하기 어렵다는 것을 강조합니다. \\n\\n둘째, 논문은 \"hard negatives\"가 LLM 성능에 미치는 영향을 규명하기 위해 다양한 retriever(정보 검색 알고리즘)와 LLM 조합을 사용한 실험을 수행합니다. 실험 결과, retriever의 성능이 높을수록 \"hard negatives\"의 영향이 더욱 커지는 것을 확인했습니다. \\n\\n셋째, 논문은 \"hard negatives\"의 문제를 해결하기 위해 \"retrieval reordering\" (정보 검색 결과의 순서 재배치)라는 새로운 방법을 제안합니다. 이 방법은 LLM이 \"lost-in-the-middle\" 현상(중간 정보에 대한 주의력 부족)을 극복하고, 가장 관련성이 높은 정보를 먼저 처리하도록 유도하여 \"hard negatives\"의 영향을 줄입니다. \\n\\n마지막으로, 논문은 \"data-augmented fine-tuning\" (데이터 증강을 통한 fine-tuning)을 통해 LLM의 \"hard negatives\"에 대한 내성을 향상시킬 수 있다는 가능성을 제시합니다. 이 방법은 LLM을 다양한 컨텍스트에서 학습시켜 \"hard negatives\"를 효과적으로 처리하도록 돕습니다. \\n\\n\\n',\n",
       " '본 논문은 Retrieval Augmented Generation(RAG)에 적합한 대규모 언어 모델(LLM)을 위한 데이터 증강 미세 조정 방법을 제시하고 그 효과를 분석합니다. \\n\\n첫째, 논문은 단순히 질문과 답변 쌍으로 미세 조정하는 것보다, 질문, 지시, 검색된 텍스트 패스지, 질문을 입력으로 받아 답변을 생성하는 RAG 특화 미세 조정을 통해 LLM의 견고성을 향상시킬 수 있다고 주장합니다. 이 방법은 다양한 검색된 텍스트를 학습시켜 LLM이 노이즈 속에서도 관련 정보를 효과적으로 식별하고 활용할 수 있도록 돕습니다. 실험 결과, RAG 특화 미세 조정은 검색된 텍스트의 수가 증가함에 따라 성능이 더욱 안정적으로 향상되는 것을 보여주며, 일반적인 질문-답변 미세 조정보다 우수한 성능을 보입니다.\\n\\n둘째, 논문은 LLM이 검색된 텍스트에서 관련 정보를 명확하게 식별하고 활용할 수 있도록, 미세 조정 과정에 중간 단계로 추론 단계를 추가하는 방법을 제안합니다. LLM은 질문과 검색된 텍스트를 입력으로 받아 관련 정보를 명시적으로 식별하는 추론 문단과 답변을 생성합니다. 이 방법은 LLM이 검색된 텍스트를 구조화된 방식으로 처리하고 관련 정보를 더욱 효과적으로 파악할 수 있도록 돕습니다. 실험 결과, 추론 단계를 추가한 미세 조정은 단순히 RAG 특화 미세 조정보다 더욱 향상된 성능을 보여줍니다.\\n\\n마지막으로, 논문은 RAG 특화 미세 조정의 효과를 높이기 위해 다양한 데이터 분포, 검색 알고리즘, 학습 텍스트 길이 등을 조사합니다. 다양한 데이터 분포를 학습시키면 LLM의 일반화 능력이 향상되며, 여러 검색 알고리즘을 사용하여 미세 조정하면 새로운 검색 알고리즘에서도 좋은 성능을 보입니다. 또한, 최대 텍스트 길이를 학습 범위로 설정하면 다양한 검색된 텍스트 수에 대해서도 안정적인 성능을 보입니다.\\n\\n\\n',\n",
       " '본 논문은 긴 텍스트 컨텍스트를 가진 대규모 언어 모델(LLM)이 검색 증강 생성(RAG) 시스템에서 장거리 입력에 대한 과제를 해결하는 방법을 연구합니다. 기존 연구와 달리, 본 논문은 많은 검색 결과를 사용하는 것이 성능 향상에 도움이 되지 않고 오히려 성능 저하를 초래할 수 있다는 것을 발견했습니다. 이는 검색된 \"hard negatives\"가 LLM의 성능에 부정적인 영향을 미치기 때문입니다. 이 문제를 해결하기 위해, 본 논문에서는 학습 전 검색 결과 재정렬, RAG 특화 암묵적 LLM 미세 조정, 그리고 중간 추론을 포함한 RAG 지향적 LLM 미세 조정 등 세 가지 해결책을 제안하고 평가합니다. 또한, 데이터 분포, 학습에 사용되는 검색 엔진, 그리고 학습 컨텍스트 길이와 같은 요소들이 RAG 성능에 미치는 영향을 분석합니다. \\n\\n본 논문은 다음과 같은 주요 내용을 다룹니다.\\n\\n* 많은 검색 결과를 사용하는 것이 LLM 성능을 향상시키지 않고 오히려 저하시킬 수 있다는 것을 발견했습니다.\\n* 이러한 성능 저하의 원인은 검색된 \"hard negatives\" 때문임을 밝혔습니다.\\n* 학습 전 검색 결과 재정렬, RAG 특화 암묵적 LLM 미세 조정, 그리고 중간 추론을 포함한 RAG 지향적 LLM 미세 조정 등 세 가지 해결책을 제안했습니다.\\n* 데이터 분포, 학습에 사용되는 검색 엔진, 그리고 학습 컨텍스트 길이와 같은 요소들이 RAG 성능에 미치는 영향을 분석했습니다.\\n* 미래 연구 방향으로는 더욱 고급화된 검색 결과 정렬 방법을 사용한 자동화된 위치 최적화와 다단계 추론 체인을 위한 LLM 미세 조정을 제시했습니다.\\n\\n\\n\\n',\n",
       " '본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)이 Retrieval Augmented Generation(RAG)과 같은 기존 기법과 어떻게 상호 작용하는지, 그리고 장문 입력에 대한 RAG의 새로운 가능성을 탐구합니다. \\n\\n첫째, 연구자들은 장문 맥락 LLM이 RAG에 적용될 때 발생하는 새로운 과제들을 분석합니다. 특히, 장문 맥락 LLM은 많은 양의 정보를 처리해야 하기 때문에, 검색된 텍스트의 양과 질이 RAG 성능에 큰 영향을 미칠 수 있습니다. \\n\\n둘째, 연구자들은 다양한 LLM과 검색 알고리즘을 사용하여 NQ와 PopQA와 같은 데이터셋에서 실험을 수행합니다. 실험 결과, 강력한 검색 알고리즘(e5)을 사용할 때 LLM은 초기에는 성능이 향상되지만, 검색된 텍스트의 양이 증가함에 따라 성능이 감소하는 경향을 보입니다. 반면, 약한 검색 알고리즘(BM25)을 사용할 때는 검색된 텍스트의 양이 증가함에 따라 성능이 지속적으로 향상됩니다. \\n\\n셋째, 연구자들은 LLM이 \"hard negative\" (정답이 없는 관련성 있는 텍스트)에 취약하다는 것을 발견합니다. 특히, 강력한 검색 알고리즘으로 검색된 \"hard negative\"는 LLM의 성능에 더 큰 영향을 미칩니다. \\n\\n넷째, 연구자들은 장문 맥락 LLM의 RAG 성능을 향상시키기 위한 몇 가지 방법을 제안합니다. 예를 들어, \"hard negative\"를 제거하거나, 검색된 텍스트의 질을 향상시키는 방법을 고려할 수 있습니다. \\n\\n마지막으로, 연구자들은 장문 맥락 LLM과 RAG의 잠재력을 강조하며, 앞으로 이 분야에서 더 많은 연구가 필요하다고 주장합니다.\\n\\n\\n',\n",
       " '본 논문은 장문 텍스트 입력에 대한 Retrieval Augmented Generation(RAG) 시스템의 성능을 향상시키기 위한 방법을 제시합니다. 특히, 장문 텍스트를 효과적으로 처리하고 이해하기 위해 Long-Context LLMs(대용량 언어 모델)을 RAG에 적용하는 방법에 초점을 맞춥니다. \\n\\n논문은 첫째, 장문 텍스트 입력에 대한 RAG 시스템의 기존 문제점을 분석합니다. 둘째, 이러한 문제점을 해결하기 위해 Long-Context LLMs을 활용한 새로운 RAG 프레임워크를 제안합니다. \\n\\n셋째, 제안된 프레임워크가 다양한 데이터셋에서 기존 RAG 시스템보다 우수한 성능을 보여준다는 것을 실험을 통해 입증합니다. 넷째, Long-Context LLMs의 장점을 활용하여 질문에 대한 답변을 생성하는 과정에서 더욱 정확하고 관련성 있는 결과를 얻을 수 있다는 것을 보여줍니다. 다섯째, 논문은 Hard Negative 문제를 다루며, LLMs가 답변과 관련이 없는 텍스트에 영향을 받지 않도록 하는 방법을 제시합니다. 마지막으로, Long-Context LLMs를 RAG에 적용함으로써 정보 검색 및 질문 답변 시스템의 성능을 향상시킬 수 있다는 결론을 내립니다.\\n\\n\\n',\n",
       " '본 논문은 긴 입력에 대한 RAG(Retrieval Augmented Generation)의 새로운 접근 방식을 제시하며, 특히 긴 텍스트 컨텍스트에서의 성능 향상에 초점을 맞춥니다. \\n\\n첫째, 논문은 긴 텍스트 컨텍스트에서의 RAG의 어려움을 분석하고, 이를 극복하기 위한 다양한 방법을 제안합니다. 둘째, 논문은 긴 입력에 대한 RAG 모델의 성능을 향상시키기 위해 새로운 데이터셋과 훈련 방법을 소개합니다. 셋째, 논문은 제안된 방법이 기존 방법에 비해 뛰어난 성능을 보여준다는 것을 실험 결과를 통해 입증합니다. 넷째, 논문은 긴 텍스트 컨텍스트에서의 RAG 모델의 성능 향상을 위한 추가적인 연구 방향을 제시합니다. 다섯째, 논문은 긴 입력에 대한 RAG 모델의 성능 향상을 위한 실용적인 방법을 제공합니다. 마지막으로, 논문은 긴 텍스트 컨텍스트에서의 RAG 모델의 성능 향상에 대한 심층적인 분석을 제공합니다. \\n\\n\\n',\n",
       " '본 논문은 Long-Context LLMs를 활용한 RAG(Retrieval Augmented Generation) 모델의 성능 향상을 다룹니다. 특히, 중간 단계의 추론 능력을 강화하여 질문에 대한 답변을 생성하는 데 초점을 맞춥니다. \\n\\n본 논문은 다양한 데이터셋(NQ, Wizard of Wikipedia, FEVER, MMLU)을 사용하여 RAG 모델을 학습하고 평가합니다. 학습 과정에서, 모델은 질문과 관련된 문서를 분석하고, 답변을 생성하기 위한 논리적 근거를 제시하는 능력을 향상시키도록 지도 학습됩니다. \\n\\n평가 결과, 중간 단계 추론을 활용한 RAG 모델은 기존의 RAG 모델보다 질문에 대한 답변의 정확도와 완전성을 향상시키는 것을 보여줍니다. \\n\\n특히, 긴 텍스트 입력에 대한 처리 능력이 향상되었으며, 복잡한 질문에 대한 답변을 생성하는 데에도 효과적임을 확인했습니다. \\n\\n논문은 또한 데이터 증강 기법을 활용하여 RAG 모델의 성능을 더욱 향상시킬 수 있는 가능성을 제시합니다. \\n\\n결론적으로, 본 논문은 Long-Context LLMs를 활용한 RAG 모델의 성능 향상을 위한 새로운 접근 방식을 제시하며, 긴 텍스트 입력 처리 및 복잡한 질문 답변에 대한 잠재력을 보여줍니다.\\n\\n\\n',\n",
       " '본 논문은 Gemma-2-9B와 같은 대규모 언어 모델(LLM)을 사용하여 질의응답(RAG) 시스템을 개선하는 방법에 대해 다룹니다. 특히, 데이터 증강된 RAG fine-tuning 기법이 LLM의 성능을 향상시키는 데 효과적임을 보여줍니다. \\n\\n첫째, 논문은 다양한 데이터셋에서 Gemma-2-9B 모델을 RAG fine-tuning하여 그 성능을 평가합니다. RAG fine-tuning은 일반적인 챗 모델보다 질의응답 작업에서 더 높은 정확도를 보입니다. 둘째, 논문은 RAG fine-tuning에 중간 단계의 추론을 추가하여 모델의 성능을 더욱 향상시키는 방법을 제시합니다. 이 방법은 복잡한 질문에 대한 답변을 생성하는 데 유용합니다. 셋째, 논문은 Mistral-Nemo-12B 모델과 같은 다른 LLM에서도 데이터 증강된 RAG fine-tuning 기법이 효과적임을 보여줍니다. 넷째, 논문은 RAG fine-tuning이 LLM의 장점을 활용하여 질의응답 시스템의 성능을 향상시키는 데 유용한 방법임을 강조합니다. 마지막으로, 논문은 앞으로의 연구 방향으로 데이터 증강 기법의 다양한 적용 및 더욱 복잡한 질문에 대한 답변을 생성하는 방법에 대한 연구를 제안합니다.\\n\\n\\n',\n",
       " '본 논문은 긴 텍스트 입력에 대한 RAG(Retrieval Augmented Generation)의 효율성을 높이기 위해 Long-Context LLM(Large Language Model)을 사용하는 방법을 연구합니다. \\n\\n첫째, Mistral-Nemo-12B와 Gemini-1.0-Pro 모델을 사용하여 RAG-specific fine-tuning의 효과를 보여줍니다. \\n\\n둘째, RAG-specific fine-tuning 데이터의 크기가 LLM 성능에 미치는 영향을 분석하여 데이터 규모가 증가함에 따라 성능이 향상됨을 확인합니다. \\n\\n셋째, 일반적인 SFT(Supervised Fine-Tuning) 데이터와 RAG-specific 데이터를 결합하여 LLM을 학습시키고, 이 방법이 RAG 성능을 향상시키면서도 일반적인 언어 처리 능력을 유지할 수 있음을 보여줍니다. \\n\\n결론적으로, 본 논문은 긴 텍스트 입력을 처리하는 RAG 시스템에서 Long-Context LLM을 활용하고, RAG-specific fine-tuning과 데이터 규모 조절을 통해 성능을 향상시킬 수 있는 방법을 제시합니다. \\n\\n\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ckS-TZVsMpoa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 장문 맥락 LLM을 활용한 RAG 시스템 성능 향상 방안 연구\n",
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 Retrieval-Augmented Generation(RAG) 시스템에 적용하여 성능을 향상시키는 방법을 연구합니다. \n",
      "\n",
      "**핵심 내용:**\n",
      "\n",
      "* **장문 맥락 LLM의 RAG 적용 문제점:** 장문 맥락 LLM은 많은 정보를 처리해야 하기 때문에, 검색된 텍스트의 양과 질이 RAG 성능에 큰 영향을 미칩니다. 특히, 강력한 검색 알고리즘으로 검색된 \"hard negatives\" (정답과 관련 없는 텍스트)는 LLM의 성능을 저하시킬 수 있습니다.\n",
      "* **해결 방안:** \n",
      "    * 검색된 문서의 순서를 재정렬하여 LLM이 관련성 높은 정보에 집중하도록 유도\n",
      "    * hard negatives에 대한 내성을 갖도록 LLM을 훈련\n",
      "    * LLM이 검색된 문서를 분석하고 관련 정보를 명시적으로 식별하도록 훈련\n",
      "* **실험 결과:** 제안된 방법들은 다양한 데이터셋과 검색 알고리즘에서 성능 향상을 보여주며, 특히 긴 텍스트 입력에 대한 처리 능력을 향상시키는 데 효과적입니다.\n",
      "\n",
      "**결론:**\n",
      "\n",
      "장문 맥락 LLM을 RAG 시스템에 적용하면 긴 텍스트 입력 처리 능력을 향상시킬 수 있으며, 데이터 분포, 사용된 검색 알고리즘, 훈련 텍스트 길이 등 다양한 요소를 고려하여 성능을 극대화할 수 있습니다.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reduce 과정 : 각 문서의 요약을 하나로 합칩니다.\n",
    "reduce_prompt = ChatPromptTemplate.from_messages([\n",
    "\n",
    "    ('system', '''\n",
    "Generate a summary of the following text that includes the following elements:\n",
    "\n",
    "* A title that accurately reflects the content of the text.\n",
    "* An introduction paragraph that provides an overview of the topic.\n",
    "* Bullet points that list the key points of the text.\n",
    "* A conclusion paragraph that summarizes the main points of the text.\n",
    "\n",
    "Answer in Korean.\n",
    "'''),\n",
    "    ('user', '''{text}\n",
    "---\n",
    "요약(In Korean):\n",
    "''')])\n",
    "\n",
    "reduce_chain = reduce_prompt | gemma2 | StrOutputParser()\n",
    "\n",
    "summary = reduce_chain.invoke('\\n---\\n'.join(raw_summaries))\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pi84F2cYMpoa"
   },
   "source": [
    "## Refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZfANrVtMpoa"
   },
   "source": [
    "Refine은 청크를 순서대로 참고하며, 매 시점 요약문을 작성합니다.   \n",
    "요약문과 새로운 청크를 비교하여, 요약문을 업데이트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7dee-KnHMpob"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. \n",
      "\n",
      "장문 맥락 LLM은 더 많은 정보를 활용할 수 있어 성능 향상을 기대할 수 있지만, 실험 결과는 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. \n",
      "\n",
      "이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법과, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "결론적으로, 본 논문은 장문 맥락 LLM을 사용하는 RAG 시스템에서 발생하는 새로운 문제점을 분석하고, 이를 해결하기 위한 효과적인 방법을 제시합니다.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 청크 순서대로 각 청크들을 요약하고, 앞부분 청크 요약과 현재 청크를 연결하여 요약을 업데이트하며 진행한다.\n",
    "# 시간은 오래 걸릴 수 있으나, 청크 업데이트가 잦은 경우에 사용\n",
    "first_summarize_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''당신은 인공지능 전문가입니다.\n",
    "주어진 논문의 내용을 읽고 한국어로 요약하세요.\n",
    "요약은 1개의 문단과 문단별 5개의 문장으로 작성하세요.\n",
    "답변은 한국어로 작성하세요.'''),\n",
    "    ('user', '''{text}''')])\n",
    "\n",
    "\n",
    "\n",
    "X = first_summarize_prompt.format_messages(text=document_list[0])\n",
    "\n",
    "intermediate_summary = gemma2.invoke(X).content\n",
    "print(intermediate_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bBpA1i76Mpob"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:08,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용할 수 있어 성능 향상을 기대할 수 있지만, 실험 결과는 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. \n",
      "\n",
      "특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도, 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. 이는 단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다.\n",
      "\n",
      "이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법과, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:11<00:51,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용할 수 있어 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도, 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법과, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "새로운 내용에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:26<01:14, 10.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용할 수 있어 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법과, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "새로운 내용에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "본 논문에서는 RAG-specific fine-tuning 방법을 통해 LLM의 hard negatives에 대한 내성을 높이고, 관련성 있는 정보를 명확히 식별하도록 학습시키는 방법을 제안합니다. 이를 통해 LLM의 RAG 성능을 향상시킬 수 있습니다. 또한, 다양한 데이터 분포와 검색 알고리즘의 영향을 분석하여 RAG fine-tuning의 효과적인 전략을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:44<01:20, 13.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용할 수 있어 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법과, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "새로운 내용에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "본 논문에서는 RAG-specific fine-tuning 방법을 통해 LLM의 hard negatives에 대한 내성을 높이고, 관련성 있는 정보를 명확히 식별하도록 학습시키는 방법을 제안합니다. 이를 통해 LLM의 RAG 성능을 향상시킬 수 있습니다. 또한, 다양한 데이터 분포와 검색 알고리즘의 영향을 분석하여 RAG fine-tuning의 효과적인 전략을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [01:01<01:12, 14.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##  장문 맥락 LLM을 활용한 RAG 시스템의 문제점과 해결 방안: 새로운 내용 반영\n",
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용하여 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법과, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "새로운 연구 결과에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. 본 논문에서는 RAG-specific fine-tuning 방법을 통해 LLM의 hard negatives에 대한 내성을 높이고, 관련성 있는 정보를 명확히 식별하도록 학습시키는 방법을 제안합니다. 이를 통해 LLM의 RAG 성능을 향상시킬 수 있습니다. 또한, 다양한 데이터 분포와 검색 알고리즘의 영향을 분석하여 RAG fine-tuning의 효과적인 전략을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [01:20<01:04, 16.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## 장문 맥락 LLM을 활용한 RAG 시스템의 문제점과 해결 방안: 새로운 내용 반영\n",
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용하여 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "새로운 연구 결과에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [01:34<00:46, 15.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## 장문 맥락 LLM을 활용한 RAG 시스템의 문제점과 해결 방안: 새로운 내용 반영\n",
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용하여 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "새로운 연구 결과에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "**추가적으로, 논문에서는 다양한 데이터셋과 평가 방법을 사용하여 실험을 진행했습니다.** \n",
      "\n",
      "* **훈련 데이터셋:** Natural Question, Wizard of Wikipedia, FEVER, MMLU 등 다양한 유형의 데이터셋을 사용하여 LLM의 성능을 향상시키는 방법을 연구했습니다.\n",
      "* **테스트 데이터셋:** TriviaQA, PopQA, WebQuestions, HotpotQA, 2WikiMultiHopQA, Bamboogle, ASQA, T-REx, Zero-shot RE 등 다양한 RAG 관련 테스트 데이터셋을 사용하여 제안된 방법의 효과를 평가했습니다.\n",
      "* **평가 방법:** 기존의 평가 방법을 개선하여 \"hard negatives\" 문제를 고려한 새로운 평가 방법을 제안했습니다.\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [01:50<00:31, 15.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## 장문 맥락 LLM을 활용한 RAG 시스템의 문제점과 해결 방안: 새로운 내용 반영\n",
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용하여 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "새로운 연구 결과에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아니며, 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "**추가적으로, 논문에서는 다양한 데이터셋과 평가 방법을 사용하여 실험을 진행했습니다.** \n",
      "\n",
      "* **훈련 데이터셋:** Natural Question, Wizard of Wikipedia, FEVER, MMLU 등 다양한 유형의 데이터셋을 사용하여 LLM의 성능을 향상시키는 방법을 연구했습니다.\n",
      "* **테스트 데이터셋:** TriviaQA, PopQA, WebQuestions, HotpotQA, 2WikiMultiHopQA, Bamboogle, ASQA, T-REx, Zero-shot RE 등 다양한 RAG 관련 테스트 데이터셋을 사용하여 제안된 방법의 효과를 평가했습니다.\n",
      "* **평가 방법:** 기존의 평가 방법을 개선하여 \"hard negatives\" 문제를 고려한 새로운 평가 방법을 제안했습니다. \n",
      "\n",
      "**특히, 논문에서는 Slot Filling과 같은 구체적인 RAG 유형에 대한 분석과 평가 방법을 제시합니다.** \n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [02:11<00:17, 17.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## 장문 맥락 LLM을 활용한 RAG 시스템의 문제점과 해결 방안: 새로운 내용 반영\n",
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. 장문 맥락 LLM은 더 많은 정보를 활용하여 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. \n",
      "\n",
      "연구 결과, 많은 장문 맥락 LLM에서 검색된 정보의 양이 증가할수록 생성된 출력의 품질은 처음에는 향상되지만, 이후 감소하는 경향을 보입니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. 특히, 강력한 검색 알고리즘이 더 많은 관련 정보를 찾아내더라도 그 안에 포함된 \"hard negatives\"가 LLM의 성능을 저해하는 문제점이 발견되었습니다. \n",
      "\n",
      "새로운 연구 결과에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아닙니다. 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "**추가적으로, 논문에서는 다양한 데이터셋과 평가 방법을 사용하여 실험을 진행했습니다.** \n",
      "\n",
      "* **훈련 데이터셋:** Natural Question, Wizard of Wikipedia, FEVER, MMLU 등 다양한 유형의 데이터셋을 사용하여 LLM의 성능을 향상시키는 방법을 연구했습니다.\n",
      "* **테스트 데이터셋:** TriviaQA, PopQA, WebQuestions, HotpotQA, 2WikiMultiHopQA, Bamboogle, ASQA, T-REx, Zero-shot RE 등 다양한 RAG 관련 테스트 데이터셋을 사용하여 제안된 방법의 효과를 평가했습니다.\n",
      "* **평가 방법:** 기존 평가 방법을 개선하여 \"hard negatives\" 문제를 고려한 새로운 평가 방법을 제안했습니다. \n",
      "\n",
      "**특히, 논문에서는 Slot Filling과 같은 구체적인 RAG 유형에 대한 분석과 평가 방법을 제시합니다.** \n",
      "\n",
      "**예를 들어, \"Fidelity Fiduciary Bank\" 질문에 대한 답변을 생성하는 과정을 설명하며, LLM이 \"hard negatives\"에 영향을 받는 것을 보여줍니다. 또한, 논문은 다양한 데이터셋과 평가 방법을 사용하여 실험을 진행했으며, 특히 Slot Filling과 같은 구체적인 RAG 유형에 대한 분석과 평가 방법을 제시합니다. \n",
      "\n",
      "\n",
      "\n",
      "---\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [02:37<00:00, 15.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## 장문 맥락 LLM을 활용한 RAG 시스템의 문제점과 해결 방안: 새로운 내용 반영\n",
      "\n",
      "본 논문은 장문 맥락을 가진 대규모 언어 모델(LLM)을 사용하는 Retrieval-Augmented Generation(RAG) 시스템의 문제점과 해결책을 제시합니다. \n",
      "\n",
      "장문 맥락 LLM은 더 많은 정보를 활용하여 성능 향상을 기대할 수 있지만, 실험 결과 그렇지 않은 경우가 많았습니다. 특히, 검색된 정보의 양이 증가할수록 생성된 출력의 품질이 처음에는 향상되지만, 이후 감소하는 경향을 보이는 문제점이 발견되었습니다. 이는 검색된 \"hard negatives\" (관련성이 낮은 정보)가 LLM 생성에 부정적인 영향을 미치기 때문으로 분석되었습니다. \n",
      "\n",
      "새로운 연구 결과에 따르면, \"hard negatives\"는 LLM의 성능에 큰 영향을 미치며, 특히 강력한 검색 알고리즘이 사용될수록 더욱 심각해집니다. 기존의 평가 방법은 주로 무작위 음성을 사용하기 때문에, 실제 RAG 시스템에서 발생하는 \"hard negatives\" 문제를 충분히 반영하지 못합니다. 따라서, \"hard negatives\"를 고려한 새로운 평가 방법이 필요합니다. \n",
      "\n",
      "단순히 검색된 정보의 양이 많을수록 성능이 향상되는 것은 아닙니다. 정보의 질과 관련성이 중요함을 시사합니다. 이러한 문제를 해결하기 위해 논문에서는 학습 없이 검색된 문서 순서를 재정렬하는 방법, LLM을 학습시켜 hard negatives에 대한 내성을 높이는 방법, 그리고 LLM이 검색된 정보 중 관련성 있는 정보를 명시적으로 식별하도록 학습시키는 방법을 제안합니다. \n",
      "\n",
      "논문에서는 다양한 데이터셋과 평가 방법을 사용하여 실험을 진행했습니다. \n",
      "\n",
      "* **훈련 데이터셋:** Natural Question, Wizard of Wikipedia, FEVER, MMLU 등 다양한 유형의 데이터셋을 사용하여 LLM의 성능을 향상시키는 방법을 연구했습니다.\n",
      "* **테스트 데이터셋:** TriviaQA, PopQA, WebQuestions, HotpotQA, 2WikiMultiHopQA, Bamboogle, ASQA, T-REx, Zero-shot RE 등 다양한 RAG 관련 테스트 데이터셋을 사용하여 제안된 방법의 효과를 평가했습니다.\n",
      "* **평가 방법:** 기존 평가 방법을 개선하여 \"hard negatives\" 문제를 고려한 새로운 평가 방법을 제안했습니다. \n",
      "\n",
      "특히, 논문에서는 Slot Filling과 같은 구체적인 RAG 유형에 대한 분석과 평가 방법을 제시합니다. 예를 들어, \"Fidelity Fiduciary Bank\" 질문에 대한 답변을 생성하는 과정을 설명하며, LLM이 \"hard negatives\"에 영향을 받는 것을 보여줍니다. \n",
      "\n",
      "또한, 논문은 Mistral-Nemo-12B와 Gemini-1.0-Pro와 같은 다양한 LLM 모델을 사용하여 실험을 진행했으며, RAG-specific fine-tuning의 효과를 보여줍니다. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "=======================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Refine Prompt\n",
    "\n",
    "refine_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', '''당신은 인공지능 전문가입니다.\n",
    "논문의 현재 시점까지의 한국어 요약이 주어집니다.\n",
    "이를 읽고, 새롭게 주어지는 내용과 비교하여 한국어 요약을 보완하거나 수정하세요.\n",
    "전체 요약은 10문장 이내로 작성하세요.\n",
    "'''),\n",
    "    ('user', '''현재 시점까지의 요약: {previous_summary}\n",
    "---\n",
    "새로운 내용: {new_text}''')])\n",
    "\n",
    "\n",
    "refine_chain = refine_prompt | gemma2 | StrOutputParser()\n",
    "\n",
    "for i in tqdm(range(1, len(document_list))):\n",
    "    intermediate_summary = refine_chain.invoke(\n",
    "        {'previous_summary':intermediate_summary,\n",
    "         'new_text':document_list[i].page_content})\n",
    "    print('')\n",
    "    print(intermediate_summary)\n",
    "    print('=======================')\n",
    "# 길이를 지정하지 않으면 오래 걸릴 수 있습니다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
